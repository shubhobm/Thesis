\section{Introduction}\label{sec:scatter-sec1-intro}
%Suppose we have data $X_{1}, \ldots, X_{n}$ from some subset $\cX$ of a real separable Hilbert $\cH$, we consider two functions defined below.

Consider a real separable Hilbert space $\cH$, and the following two functions. Firstly the {\it sign function} $S : \cH \times \cH \rightarrow \cH$, which is defined as
%
\begin{align}\label{eqn:signEqn}
S (x; \mu_{x}) = \frac{x - \mu_{x}}{\| x - \mu_{x} \|} {\BI}_{ x \ne \mu_{x} }
\end{align}
%
with respect to the {\it location parameter} $\mu_{x} \in \cH$, and the norm $\| \cdot \|$ used above is the norm of the underlying Hilbert space. This is a direct  generalization of the real-valued case of the indicator of whether the point $x$ is to the right, left or at $\mu_{x}$. This function had first been introduced by \cite{MottonenOja95}, and has seen widespread application in robust statistics across the past two decades \citep{LocantoreEtal99, OjaBook10, WangPengLi15}.

Next we describe the {\it peripherality function}, for which some mathematical preliminaries are necessary for easier exposition. Let $(\Omega, \cA, \alpha)$ be a probability space, and let $\cB$ be the Borel $\sigma$-algebra generated by the norm topology of $\cH$. A $\cH$-valued random variable is a mapping $X :  \Omega \mapsto \cH$ such that for every $B \in \cB$, $X^{-1} (B) \in \cA$. It is easy to see that $\alpha_{x} = \alpha ( X^{-1} (\cdot ) )$ is a probability measure on the measurable space $(\cH, \cB)$. Mathematical details about such probability measures on Hilbert spaces are available from a number of places \citep{Segal58,Gross67}.
%including BLSP notes (my primary reference), Gross, Segal, and what not.

Let $\cM$ be a set of probability measures on $\cH$. 
%When $\cH$ 
%is infinite-dimensional, we further impose the conditions that for any $\nu \in \cM$
%\ban 
%\int_{\cH} || x || \nu (d x) < \infty, \hspace{1 cm} 
%\int_{\cH} || x ||^{2} \nu (d x) < \infty.
%\end{align}
A {\it peripherality function} 
$ P : \cH \times \cM \rightarrow \BR$,  is a function that satisfies the following 
condition:\\
{\it For every probability measure $F \in \cM$, there exists a constant $\mu_{F} \in \cH$ 
such that for every $t \in [ 0, 1]$ and every $x \in \cH$}
\begin{align*} 
P \Bigl( \mu_{F} ; F \Bigr) \leq P \Bigl( \mu_{F} + t ( x - \mu_{F} ); F \Bigr). 
\end{align*} 
%
That is, for every fixed $F$, the peripherality function achieves a minimum at $\mu_{F}$, and is non-decreasing in every direction away from $\mu_{F}$. If we impose the practical restriction that $\inf_{x} P ( x ; F )$ is finite and bounded below, then we may as well impose without loss of generality $P ( \mu_{F} ; F ) = 0$ and consequently $P ( x ; F ) \geq 0$ for all $x \in \cH$ and  $F \in \cM$. In many cases of interest, $P ( \cdot; \cdot)$ is uniformly bounded above as well.

The peripherality function quantifies whether the point $x$ is near or far from $\mu_{F}$. 
We will impose additional conditions on this function as we proceed, but it can be seem immediately that any distance measure between $x$ and $\mu_{F}$ satisfies the bare minimum requirement mentioned above.

In this chapter, we demonstrate interesting applications arising from composing the sign function and the peripherality function together, to form the {\it signed-peripherality function}. We define this function 
with three parameters $\mu_{x} \in \cH$, $F \in \cM$ and $\mu_{y} \in \cH$,  argument $x \in \cH$ and range $\cH$. More precisely, we use two functions $\kappa_{s} : \cH \rightarrow \cH$, $\kappa_{p} : \cH \rightarrow \cH$ that are respectively composed with the sign transformation and the peripherality function, and 
then multiplied together to obtain the function $\kappa : \cH \times \cH \times \cM \times \cH \times \cH \rightarrow \cH$ defined as 
%
\begin{align}\label{eqn:kappaEqn}
\kappa (x; \mu_{x}, F, \mu_{y} ) = \kappa_{s} (S (x; \mu_{x})) \kappa_{p} (P (x; F)) + \mu_{y}. 
\end{align}
%
We have deliberately set the location parameters $\mu_{x}, \mu_{F}, \mu_{y}$ to be potentially non-identical, this additional flexibility has some advantage for robust data analysis. In many applications, the value of these three parameters may be identical, which leads to no conflict in our framework.
% Also notice that if we consider $\mu_{y} = \mu_{F} = \mu_{x}$, $\kappa_{s} (x) = \kappa_{p} (x) = x$, and take the very simple peripherality function $P( x; F) = || x - \mu_{F} ||$, we have $\kappa (x; \mu_{x}, F, \mu_{y} ) \equiv x$ for all choices of parameters $\mu_{x}, F, \mu_{y}$.  Consequently, under this choice of parameters for the $\kappa$-transformation, analyzing a dataset $\{ X_{1}, \ldots, X_{n} \}$ and its $\kappa$-transformed version $\{ Y_{i} = \kappa (X_{i}; \ldots), \ i = 1, \ldots, n \}$ are equivalent. However, in this paper we illustrate how other choices of the peripherality function lead to interesting robustness results.

We are going to elaborate on the case when $\cH$ is the $p$-dimensional Euclidean space $\BR^p$ in \ref{eqn:kappaEqn} above, for some positive integer $p$. In this situation, a whole class of peripherality functions can be defined from {\it Data depth} functions.
% Data depth has seen extensive use in the past decade, ranging from robust nonparametric classification \citep{jornsten04, ghosh05, dutta12, sguera14} to parametric estimation of means \citep{ZuoCuiHe04} and covariance matrices \citep{ZuoCui05}. An overview of statistical depth functions can be found in \citep{zuo00}.
Peripherality functions can be defined as some inverse ranking based on data depth, and the concept of {\it outlyingness} associated with data depth (see \cite{zuo00}) is essentially same as what we use in this paper.  Coming back to \ref{eqn:kappaEqn}, we fix $\kappa_s(x)=x, \mu_y = {\bf 0}_p$, and shall consider two separate choices of $\kappa_p$. In \ref{sec:scatter-sec2-location} we show that when $\kappa_{p}$ is a monotonically decreasing function of its argument, it leads to favorable asymptotic and finite sample efficiency results in robust multivariate location estimation and high-dimensional testing. On the other hand, an opposite characterization of $\kappa_p(.)$, i.e. when it is an monotonically \textit{increasing} function, results in better performance compared to the spatial sign-based principal component analysis (PCA) in \ref{section:dcmSection}, as well as robustification of Sufficient Dimension Reduction \citep{AdragniCook09} in \ref{sec:scatter-sec4-sdr} and functional PCA in \ref{section:fpcaSection}.

% We use the term {\it peripherality} to keep track of the difference in application contexts and technical assumptions. As an additional objective of this paper, we discuss some properties and uses of data-depth in real, separable Hilbert spaces.

\section{The robust location problem}\label{sec:scatter-sec2-location}
%Following \cite{FangEtal90}, elliptical distributions can be formally defined using their characteristic function:
%%
%\begin{Definition}
%A $p$-dimensional random vector $\bfX$ is said to elliptically distributed if and only if there exist a vector $\bfmu \in \mathbb R^p$, a positive semi-definite matrix $\bfOmega \equiv \bfSigma^{-1} \in \mathbb R^{p \times p}$ and a function $\phi: \mathbb R_+ \rightarrow \mathbb R$ such that the characteristic function $\bft \mapsto \phi_{\bfX - \bfmu} (\bft)$ of $\bfX - \bfmu$ corresponds to $\bft \mapsto \phi (\bft^T \bfSigma \bft), \bft \in \mathbb R^p$.
%\end{Definition}
%%
%The density function of an elliptically distributed random variable takes the form:
%%
%$$ h(\bfx; \bfmu, \bfSigma) = |\bfOmega|^{1/2} g ((\bfx - \bfmu)^T \bfOmega (\bfx - \bfmu)) $$
%%
%where $g$ is a non-negative scalar-valued density function that is continuous and strictly increasing, and is called the \textit{density generator} of the elliptical distribution. For ease of notation, we shall denote such a distribution by $F := \mathcal{E} (\bfmu, \bfSigma, g)$.

Consider an elliptic distribution in $\BR^p$, denoted by $\cE (\bfmu, \bfSigma, g)$, for which we take the characterization of \cite{FangEtal90} as given in \ref{chapter:evalue-chapter}. In this section we focus on the problem of estimation and testing for the location parameter $\bfmu$ in this distribution using data-dependent weights on the spatial sign vectors:
%
\begin{align*}
\bfX_w = w( \bfX) \bfS( \bfX)
\end{align*}
%
where $\bfS (\bfx) \equiv  \bfS (\bfx; {\bf 0}_p) = \| \bfx \|^{-1} \bfx . {\BI}_{ \bfx \neq {\bf 0}_p }$, adapting the definition of spatial signs in \ref{eqn:signEqn} for $\BR^p$. For now the only condition we impose on these weights, say $w(.)$, is that they need to be scalar-valued affine invariant and square-integrable functions of $\bfX$, or equivalently of the norm of the standardized random variable $\bfZ \equiv \bfSigma^{-1/2} (\bfX - \bfmu)$. In other words, it is possible to write $w(\bfX)$ as $f(r)$, with $r = \| \bfZ \|$. Our theoretical analysis in this section assumes this general weighs structure. The role of peripherality functions \textit{vis-\'{a}-vis} the characterization in \ref{eqn:kappaEqn} comes in the form of empirical evidence, where we demonstrate better performance compared to spatial sign-based procedures when $f(r)$ is taken as a decreasing function of $r$.

The simplest use of weighted signs in the location problem would be to construct an outlier-robust alternative to the Hotelling's $T^2$ test using their sample mean vector and covariance matrix. Formally, given a size-$n$ sample $\BX_n = (\bfX_1, \ldots, \ldots \bfX_n)^T$ of independent and identically distributed (as $\bfX$) random variables, this means testing for $H_0: \bfmu = {\bf 0}_p$ vs. $H_1:\bfmu \neq {\bf 0}_p$  based on the test statistic:
%
$$ T_{n,w} = n \bar\bfX_w^T ( Cov (\bfX_w))^{-1} \bar\bfX_w $$
%
with $\bar\bfX_w = \sum_{i=1}^n \bfX_{w,i}/n$ and $\bfX_{w,i} = w(\bfX_i ) \bfS (\bfX_i)$ for $i=1,2,...,n$. However, the following holds true for this weighted sign test:
%
\begin{Proposition}\label{proposition:SignTest}
Consider $n$ random variables $\BZ_n = (\bfZ_1,...,\bfZ_n)^T$ distributed independently and identically as $\mathcal{E}( \bfmu, k\bfI_p, g); k > 0$, and the class of hypothesis tests defined above. Then, given any $\alpha \in (0,1)$, local power at $\bfmu \neq {\bf 0}_p$ for the level-$\alpha$ test  based on $T_{n,w}$ is maximum when $w(\bfZ_1) = c$, a constant independent of $\bfZ_1$.
\end{Proposition}
%
\noindent This essentially means that power-wise the (unweighted) spatial sign test \citep{OjaBook10} is optimal in the given class of hypothesis tests when the data comes from a spherically symmetric distribution. Our simulations show that this empirically holds for non-spherical elliptic distributions as well.

\subsection{The weighted spatial median} 

In order to explore usage of weighted spatial signs in the location problem that improve upon the state-of-the-art, we now concentrate on the following optimization problem:
%
\begin{equation}\label{eqn:WtSpMed}
\bfmu_w = \text{arg}\min_{\bfmu_0 \in \mathbb{R}^p} \BE ( w(\bfX) | \bfX - \bfmu_0 |)
\end{equation}
%
This can be seen as a generalization of the Fermat-Weber location problem, which has the spatial median \citep{brown83, Chaudhuri96} as its solution, using data-dependent weights. Using affine invariant weights in \ref{eqn:WtSpMed} ensures that the weights are independent of $\bfmu_0$, which allows the optimization problem to have a unique solution. We call this solution the \textit{weighted spatial median} of $F$, and denote it by $\bfmu_w$. In a sample setup it is estimated by iteratively solving the equation $\sum_{i=1}^n w(\bfX_i) \bfS (\bfX_i - \hat\bfmu_w)/n = {\bf 0}_p$.

The sample weighted spatial median $\hat\bfmu_w$ is a $\sqrt n$-consistent estimator of $\bfmu_w$, and gives its asymptotic distribution:
%
\begin{Theorem}
Let $\bfA_w, \bfB_w$ be two matrices, dependent on the weight function $w$ such that
%
$$
\bfA_w = \BE \left[ \frac{w( \bfepsilon ) }{\| \bfepsilon \|} \left( 1 - \frac{\bfepsilon \bfepsilon^T}{\| \bfepsilon \|^2} \right) \right]; \quad \bfB_w = \BE \left[ \frac{(w( \bfepsilon ))^2 \bfepsilon \bfepsilon^T}{\| \bfepsilon \|^2} \right]
$$
%
where $\bfepsilon \sim \mathcal E({\bf 0}_p, \bfSigma, g)$. Then
%
\begin{equation}
\sqrt n (\hat\bfmu_w - \bfmu_w) \leadsto N_p ({\bf 0}_p, \bfA_w^{-1} \bfB_w \bfA_w^{-1})
\end{equation}
\end{Theorem}
%

The above theorem generalizes equivalent results for the spatial median \citep{OjaBook10}, and can be proved in a similar fashion. Note that setting $w(\bfepsilon)=1$ above yields the asymptotic covariance matrix for the spatial median. Following this, the asymptotic relative efficiency (ARE) of $\bfmu_w$ corresponding to some non-uniform weight function with respect to the spatial median, say $\bfmu_s$ will be:
%
\begin{align}\label{eqn:AREEqn}
ARE( \bfmu_w, \bfmu_s) = \left[ \frac{\text{det} (\bfA^{-1} \bfB \bfA^{-1})}{\text{det} (\bfA_w^{-1} \bfB_w \bfA_w^{-1})} \right]^{1/p}
\end{align}
%
with $\bfA = \BE [ 1/ \| \bfepsilon \| ( \bfI_p - \bfepsilon \bfepsilon^T/ \| \bfepsilon \|^2 ) ]$ and $\bfB = \BE [ \bfepsilon \bfepsilon^T/ \| \bfepsilon \|^2 ]$. This is further simplified under spherical symmetry:

\begin{Corollary}\label{Corollary:wsmCorollary}
For a spherical distribution $\mathcal{E}(\bfmu, k\bfI_p, g); k \in \BR, \bfmu \in \BR^p$, we have
%
$$
\text{ARE} ( \bfmu_w, \bfmu_s) = \frac{ \left[ \BE \left( \frac{f(r)}{r} \right) \right]^2}{\BE f^2(r) \left[ \BE \left( \frac{1}{r} \right) \right]^2 }
$$
%
\end{Corollary}
%
\begin{table}[t]
\begin{footnotesize}
	\centering
    \begin{tabular}{c|ccccc}
    \hline
    & $t_3$   & $t_5$   & $t_{10}$  & $t_{20}$  & Normal \\ \hline
    $p=5$    & 1.28 & 1.20 & 1.16 & 1.14 & 1.13   \\
    $p=10$   & 1.15 & 1.10 & 1.07 & 1.07 & 1.06   \\
    $p=20$   & 1.09 & 1.05 & 1.04 & 1.03 & 1.03   \\
    $p=50$   & 1.05 & 1.02 & 1.01 & 1.01 & 1.01   \\ \hline
    \end{tabular}
    \caption{Table of $ARE(\bfmu_w; \bfmu_s)$ for different spherical distributions}
    \label{table:AREtablewsm}
\end{footnotesize}
\end{table}
%
At this point, choices of weights that are decreasing functions of $r$ lead ARE values larger than 1. For example, \ref{table:AREtablewsm} summarizes the AREs for several families of elliptic distributions, numerically calculated using 10,000 random samples, and taking $f(r) = 1/(1+r)$. It is evident from the table that the weighted spatial median outperforms its unweighted counterpart for all data dimensions and distribution families considered. While the performance is much better for small values of $p$, weighting the signs seems to have less and less effect as $p$ grows larger. Assuming a first order autoregressive (AR1) covariance structure, i.e. $\bfSigma_{ij} = \rho^{|i-j|}, \rho \in (0,1)$ also results in largely similar ARE values as those obtained in \ref{table:AREtablewsm} which assume $\bfSigma = \bfI_p$.

\subsection{A high-dimensional test of location}

It is possible to take an alternative approach to the location testing problem by using the covariance-type U-statistic $C_{n,w} = \sum_{i=1}^n \sum_{j=1}^{i-1} \bfX_{w,i}^T \bfX_{w,j}$. This class of test statistics are especially attractive since they are readily generalized to cover high-dimensional situations, i.e. when $p > n$. The Chen and Qin (CQ) high-dimensional test of location for multivariate normal $\bfX_i$ \citep{ChenQin10} is a special case of this test that uses the statistic $C_n = \sum_{i=1}^n \sum_{j=1}^{i-1} \bfX_i^T \bfX_j$, and a recent paper (\cite{WangPengLi15}, from here on referred to as WPL test) shows that one can improve upon the power of the CQ test for non-gaussian elliptical distributions by using spatial signs $\bfS(\bfX_i)$ in place of the actual variables.

Given these, and some mild regularity conditions, the following holds for our generalized test statistic $C_{n,w}$ under $H_0$ as $n,p \rightarrow \infty$:
%
\begin{equation}\label{eqn:hdtest1}
\frac{C_{n,w}}{\sqrt{\frac{n(n-1)}{2} \text{Tr}(\bfB_w^2)}} \leadsto N(0,1)
\end{equation}
%
and under contiguous alternatives $H_1: \bfmu = \bfmu_0$,
%
\begin{equation}\label{eqn:hdtest2}
\frac{C_{n,w} - \frac{n(n-1)}{2} \bfmu_0^T \bfA_w^2 \bfmu_0 (1 + o(1)) }{\sqrt{\frac{n(n-1)}{2} \text{Tr}(\bfB_w^2)}} \leadsto N(0,1)
\end{equation}
%
we provide the details behind deriving these two results in the supplementary material, which involve modified regularity conditions and sketches of proofs along the lines of \cite{WangPengLi15}.

The ARE of this test statistic with respect to its unweighted version, i.e. the WPL statistic, is expressed as:
%
$$
ARE(C_{n,w}, \text{WPL}; \bfmu_0) = \frac{\bfmu_0^T \bfA_w^2 \bfmu_0}{\bfmu_0^T \bfA^2 \bfmu_0} \sqrt\frac{\text{Tr}(\bfB^2)}{\text{Tr}(\bfB_w^2)} (1 + o(1))
$$
%
when $\bfSigma = k\bfI_p$ and $f(r) = 1/(1+r)$, this again simplifies to $\BE^2(f(r)/r)/[\BE f^2(r). \BE^2(1/r)]$. The ARE values will be exactly same as those in \ref{table:AREtablewsm}, which indicates that for large data dimension the WPL test and that based on $C_{n,w}$ are almost equivalent.

However, in a practical high-dimensional setup one almost always has to work with a low sample size. For this reason, comparing the the two tests with respect to their \textit{finite sample} efficiencies instead should give a better idea of their practical utility. We do this in \ref{table:AREtablehd}, which lists empirical powers calculated from 1000 replications of each setup under an AR1 covariance structure (with $\rho = 0.8$). While under $H_0: \bfmu = {\bf 0}_p$ all tests have similar performance, $C_{n,w}$ beats the other two under deviations from $H_0$.

\begin{table}
\begin{footnotesize}
\centering
    \begin{tabular}{cc|ccc}\hline
\multicolumn{5}{l}{$\bfmu = \text{rep}(.15,p)$}\\\hline 
 $p$  & $n$    & CQ   & WPL  & $C_{n,w}$ \\\hline 
  500 & 20 & 0.051 & 0.376 & 0.418 \\
  500 & 50 & 0.060 & 0.832 & 0.866 \\
 1000 & 20 & 0.044 & 0.541 & 0.584 \\
 1000 & 50 & 0.039 & 0.973 & 0.987 \\\hline
\multicolumn{5}{l}{$\bfmu = \text{rep}(0,p)$}\\\hline
 $p$  & $n$    & CQ   & WPL  & $C_{n,w}$ \\\hline 
  500 & 20 & 0.049 & 0.061 & 0.063 \\
  500 & 50 & 0.039 & 0.061 & 0.064 \\
 1000 & 20 & 0.042 & 0.060 & 0.063 \\
 1000 & 50 & 0.043 & 0.050 & 0.050 \\\hline
    \end{tabular}
    \caption{Table of empirical powers of level-0.05 tests for the Chen and Qin (CQ), WPL and $C_{n,w}$ statistics}
    \label{table:AREtablehd}
\end{footnotesize}
\end{table}