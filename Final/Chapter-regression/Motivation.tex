\section{Depth-based regularization}
\label{sec:regression-sec2}
We incorporate measures of data depth as a row-level penalty function in \ref{eqn:penEqn}. Specifically, we estimate the coefficient matrix $\bfB$ by solving the following constrained optimization problem:
%
\begin{equation}\label{eqn:eqn02}
\hat\bfB = \argmin_\bfB \left[ \Tr \{ (\bfY - \bfX\bfB)^T ( \bfY - \bfX\bfB) \} + \lambda_n \sum_{j=1}^p P ( \bfb_j, F) \right]
\end{equation}
%
where $P ( \bfb_j, F)$ is a function that measures the peripherality of the $j$-th row of $\bfB$ with respect to a fixed probability distribution $F$, as defined in \ref{chapter:scatter-chapter}. We refer to $F$ as the \textit{reference distribution}, and consider it fixed in the estimation process. In multitask learning, any additive penalty function of the form $P_\lambda (\bfB) = \sum_{j=1}^p \lambda p( \bfb_j)$ regularizes individual rows of the coefficient matrix by providing a control over their distance from the origin ${\bf 0}_q$ through some norm (e.g. the $l_1/l_q$ penalty: \cite{NeghabanWainwright11}), or a combination of norms (e.g. the Adaptive Multi-task Elastic-Net: \cite{ChenEtal12}). Through \ref{eqn:eqn02} we attempt to generalize this notion by proposing to regularize using the `distance' from a \textit{probability distribution} centered at the origin. Of course, any existing method of norm-based regularization arises as a special case by by using the norm (or combination of norms) as the peripherality function and taking as $F$ the degenerate distribution centered at ${\bf 0}_q$. While it is possible to use any peripherality function (or outlyingness functions, in the spirit of \cite{zuo00}) for this purpose, of special interest is the case of \textit{inverse depth} functions: $P(\bfx, F) = D^- (\bfx, F)$. Such functions essentially invert the funnel-shaped contour of the corresponding depth function (\ref{fig:depthplot}). This immediately results in row-wise nonconvex penalties, where the penalty sharply increases for smaller entries inside the row but is bounded above for large values. This is easy to visualize for $p=1$, which we show in panel a of \ref{fig:fig1}. This serves as our motivation of using data depth in regularized multitask regression.